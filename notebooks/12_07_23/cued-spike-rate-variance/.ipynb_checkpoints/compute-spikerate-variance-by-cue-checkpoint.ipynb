{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6c39c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T20:31:09.094117Z",
     "start_time": "2022-01-23T20:31:09.078531Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd39fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T20:31:09.114221Z",
     "start_time": "2022-01-23T20:31:09.096368Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set to be lower priority\n",
    "#os.nice(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7441a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1df691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def ensure_dir(path):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR = Path('/n/groups/datta/tim_sainburg/projects/CDCP_paper/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29edc857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T20:31:09.888832Z",
     "start_time": "2022-01-23T20:31:09.116242Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "#from cdcp.paths import DATA_DIR, ensure_dir\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from tqdm.autonotebook import tqdm\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c6795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from cdcp.spiketrain_analysis.spiketrain_utils import (\n",
    "#    bin_interp_points,\n",
    "#)\n",
    "def bin_interp_points(interp_points, n_bins=16, flip_bins=True):\n",
    "    \"\"\"\n",
    "    Equally divide interp points into bins\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    interp_points : [type]\n",
    "        [description]\n",
    "    n_bins : int, optional\n",
    "        [description], by default 16\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    [type]\n",
    "        [description]\n",
    "    \"\"\"\n",
    "    if n_bins == 128:\n",
    "        return interp_points\n",
    "\n",
    "    # test to ensure that this n_bins equally divdes the data\n",
    "    ips = np.arange(128)\n",
    "    bins = np.arange(0, 127, int(128 / n_bins))\n",
    "    test_bins = np.digitize(ips, bins) - 1\n",
    "    if flip_bins:\n",
    "        test_bins = n_bins - 1 - test_bins\n",
    "    unique_bins, counts = np.unique(np.digitize(ips, bins), return_counts=True)\n",
    "    # ensure that bins are equally divided\n",
    "    assert all(x == counts[0] for x in counts)\n",
    "    binned = np.digitize(interp_points, bins) - 1\n",
    "    if flip_bins:\n",
    "        binned = n_bins - 1 - binned\n",
    "    return binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08f4df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T20:31:09.913834Z",
     "start_time": "2022-01-23T20:31:09.891750Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a50515f-f888-4ef7-8c3b-6414f9331cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7770a9e-13c1-4ed1-9bdf-a9fcf197fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lht /n/groups/datta/tim_sainburg/projects/CDCP_paper/data/population_analyses_with_passive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5e533",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T20:31:24.868501Z",
     "start_time": "2022-01-23T20:31:24.841323Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"2021-10-07_16-05-58_796914\" '2021-10-06_13-30-08_899062' '2021-10-01_17-02-56_744915' \n",
    "statistics_timestamp = '2021-10-23_23-02-31_846685'# datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S_%f\")#\n",
    "statistics_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22c611",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T20:31:24.896122Z",
     "start_time": "2022-01-23T20:31:24.870955Z"
    }
   },
   "outputs": [],
   "source": [
    "birds = [\n",
    "    ('B1597', '2021-05-14_12-40-20_423998', 'kilosort2_5'),\n",
    "    ('B1188', '2021-05-31_18-52-29_558216', 'kilosort2_5'),\n",
    "    ('B1595', '2021-07-10_16-42-47_090257', 'kilosort2'),\n",
    "    ##('B1276', '2021-07-14_11-14-02_257025', 'kilosort2'),\n",
    "    ('B1426', '2021-07-14_11-29-39_657273', 'kilosort2'),\n",
    "    ('B1432', '2021-06-01_15-14-38_561892', 'kilosort2_5'),\n",
    "    ('B1170', '2021-06-01_21-01-26_519005', 'kilosort2_5'),\n",
    "    ('B1244', '2021-07-14_12-57-45_546674', 'kilosort2'),\n",
    "    ('B1593', '2021-06-28_18-13-24_826008', 'kilosort2_5'),\n",
    "    ('B1248', '2021-07-03_18-08-01_063431', 'kilosort2_5'),\n",
    "    # acute\n",
    "    #('B1279', '2021-04-12_16-27-07_289527', 'kilosort2_5'),\n",
    "    #('B1500', '2021-08-27_09-24-48_680961', 'kilosort2_5'),\n",
    "    #('B1239', '2021-05-17_22-18-51_557635', 'kilosort2'),\n",
    "    #('B1459', '2021-08-26_21-21-12_755614', 'kilosort2_5'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8b65f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T20:31:24.918123Z",
     "start_time": "2022-01-23T20:31:24.897999Z"
    }
   },
   "outputs": [],
   "source": [
    "min_playbacks = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1495e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T20:31:24.941123Z",
     "start_time": "2022-01-23T20:31:24.919834Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from cdcp.paths import DATA_DIR, ensure_dir\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from pathlib2 import Path\n",
    "from tqdm.autonotebook import tqdm\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "from sklearn.metrics.pairwise import (\n",
    "    cosine_similarity,\n",
    "    euclidean_distances,\n",
    "    manhattan_distances,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9119b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unit_spike_trains(\n",
    "    unit_recording_ids,\n",
    "    spikesorting_folder,\n",
    "    sorter,\n",
    "    unit,\n",
    "    bird,\n",
    "    save_folder=\"trial_aligned_spikes\",\n",
    "):\n",
    "    trial_aligned_spikes_list = []\n",
    "    for unit, recording_id in tqdm(\n",
    "        unit_recording_ids, desc=\"unit spike trains\", leave=False, disable=True\n",
    "    ):\n",
    "        trial_aligned_spikes_loc = (\n",
    "            DATA_DIR\n",
    "            / \"spikesorting\"\n",
    "            / save_folder\n",
    "            / bird\n",
    "            / recording_id\n",
    "            / \"{}.pickle.zip\".format(unit)\n",
    "        )\n",
    "        if trial_aligned_spikes_loc.exists():\n",
    "            trial_aligned_spikes = pd.read_pickle(trial_aligned_spikes_loc)\n",
    "            trial_aligned_spikes[\"recording_id\"] = recording_id\n",
    "\n",
    "            trial_aligned_spikes[\"stim\"] = [\n",
    "                i[:-4] if i[-4:].lower() == \".wav\" else i\n",
    "                for i in trial_aligned_spikes.stim.values\n",
    "            ]\n",
    "\n",
    "            mask = [\n",
    "                (i.split(\"_\")[-1].isnumeric() and len(i.split(\"_\")[-1]) == 3)\n",
    "                for i in trial_aligned_spikes.stim.values\n",
    "            ]\n",
    "\n",
    "            # get cue info\n",
    "            trial_aligned_spikes[\"cue\"] = [\n",
    "                i.split(\"_\")[0] if mask else np.nan\n",
    "                for i, m in zip(trial_aligned_spikes.stim.values, mask)\n",
    "            ]\n",
    "            trial_aligned_spikes[\"interp\"] = [\n",
    "                i.split(\"_\")[1] if mask else np.nan\n",
    "                for i, m in zip(trial_aligned_spikes.stim.values, mask)\n",
    "            ]\n",
    "\n",
    "            # for i, m in zip(trial_aligned_spikes.stim.values, mask):\n",
    "            #    if m:\n",
    "            #        int(i.split(\"_\")[2])\n",
    "            trial_aligned_spikes[\"interp_point\"] = [\n",
    "                np.nan if m == False else int(i.split(\"_\")[2])\n",
    "                for i, m in zip(trial_aligned_spikes.stim.values, mask)\n",
    "            ]\n",
    "            trial_aligned_spikes_list.append(trial_aligned_spikes)\n",
    "        else:\n",
    "            0\n",
    "            # print(\"{} does not have trial aligned spikes yet\".format(recording_id))\n",
    "            # print(\"\\t\", trial_aligned_spikes_loc)\n",
    "    if len(trial_aligned_spikes_list) < 1:\n",
    "        #breakme\n",
    "        return None\n",
    "    else:\n",
    "        return pd.concat(trial_aligned_spikes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_spikes_to_stim(cue, spike_times):\n",
    "    if cue == 'NC':\n",
    "        return np.sum((spike_times > 0) & (spike_times < 1))\n",
    "    else:\n",
    "        return np.sum((spike_times > 1) & (spike_times < 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29360fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x):\n",
    "    return (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "def clip(x, _min = -4, _max = 4):\n",
    "    x[x<_min] = _min\n",
    "    x[x>_max] = _max\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e391ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unit_cued_spike_rate_and_variance(\n",
    "    unit_to_analyze,\n",
    "    spikesorting_folder,\n",
    "    statistics_timestamp,\n",
    "    bird,\n",
    "    n_interp_point_bins=16,\n",
    "    recompute=False,\n",
    "    parallel=True,\n",
    "    flip_bins=True,\n",
    "    n_time_bins=100,\n",
    "    gaussian_sigma_ms=25,\n",
    "    save=True,\n",
    "    plot=False,\n",
    "):\n",
    "    # load spike trains\n",
    "    trial_aligned_spikes = get_unit_spike_trains(\n",
    "        unit_to_analyze.sort_units,\n",
    "        spikesorting_folder,\n",
    "        sorter,\n",
    "        unit_to_analyze,\n",
    "        bird=bird,\n",
    "        save_folder=\"trial_aligned_spikes_padding_100ms\",\n",
    "    )\n",
    "\n",
    "    # disclude correction trials\n",
    "    m = np.concatenate(\n",
    "        [\n",
    "            [False],\n",
    "            trial_aligned_spikes.stim.values[:-1]\n",
    "            == trial_aligned_spikes.stim.values[1:],\n",
    "        ]\n",
    "    )\n",
    "    trial_aligned_spikes = trial_aligned_spikes[m]\n",
    "\n",
    "    # subset only active, responded to trials\n",
    "    trial_aligned_spikes = trial_aligned_spikes[\n",
    "        (trial_aligned_spikes.passive == False)\n",
    "        & (trial_aligned_spikes.response.isin([\"left\", \"right\"]))\n",
    "    ]\n",
    "\n",
    "    # ensure interp point is an integer\n",
    "    trial_aligned_spikes[\"interp_point_binned\"] = bin_interp_points(\n",
    "        trial_aligned_spikes[\"interp_point\"].values.astype(int),\n",
    "        16,\n",
    "        flip_bins=True,\n",
    "    )\n",
    "\n",
    "    trial_aligned_spikes[\"stim_id\"] = [\n",
    "        f\"{row.cue}_{row.interp_point_binned}\"\n",
    "        for idx, row in trial_aligned_spikes.iterrows()\n",
    "    ]\n",
    "\n",
    "    # compute the number of spikes to a given stimulus\n",
    "    trial_aligned_spikes[\"n_spikes_to_stim\"] = [\n",
    "        get_n_spikes_to_stim(row.cue, np.array(row.spike_times))\n",
    "        for idx, row in trial_aligned_spikes.iterrows()\n",
    "    ]\n",
    "\n",
    "    # subset_only_relevant_columns\n",
    "    trial_aligned_spikes = trial_aligned_spikes[\n",
    "        [\n",
    "            \"cue\",\n",
    "            \"stim_id\",\n",
    "            \"n_spikes_to_stim\",\n",
    "            \"passive\",\n",
    "            \"interp_point\",\n",
    "            \"interp_point_binned\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # get which trials are cue valid vs invalid\n",
    "    cue_valid_mask = (\n",
    "        (trial_aligned_spikes.interp_point < 64)\n",
    "        & (trial_aligned_spikes.cue.isin([\"CL0\", \"CL1\"]))\n",
    "    ) | (\n",
    "        (trial_aligned_spikes.interp_point >= 64)\n",
    "        & (trial_aligned_spikes.cue.isin([\"CR0\", \"CR1\"]))\n",
    "    )\n",
    "    uncued_mask = trial_aligned_spikes.cue.isin([\"NC\", \"CN\"])\n",
    "    trial_aligned_spikes[\"cue_valid\"] = \"invalid\"\n",
    "    trial_aligned_spikes.loc[uncued_mask, \"cue_valid\"] = \"uncued\"\n",
    "    trial_aligned_spikes.loc[cue_valid_mask, \"cue_valid\"] = \"valid\"\n",
    "\n",
    "    # get the mean and variance spikerate for that unit\n",
    "    summary_stats_unit = (\n",
    "        trial_aligned_spikes.groupby([\"cue_valid\", \"stim_id\"])[\"n_spikes_to_stim\"]\n",
    "        .agg([\"mean\", \"var\", \"count\"])\n",
    "        .reset_index()\n",
    "    )\n",
    "    summary_stats_unit[\"unit\"] = f\"{bird}_{unit_to_analyze.cluster_id}\"\n",
    "    \n",
    "    return summary_stats_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66240db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3376b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_units_df = pd.read_pickle(DATA_DIR / 'categorical_unit_01_23_22.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6db4e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_units_df= categorical_units_df[categorical_units_df.categorical_unit == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec733c-f86c-4a0b-a77d-bea2437d89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79844f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bird, timestamp, sorter in tqdm(birds):\n",
    "    spikesorting_folder = (DATA_DIR\n",
    "                / \"spikesorting\"\n",
    "                / bird\n",
    "                / timestamp)\n",
    "    # get the summary of recordings for the bird\n",
    "    recording_summary_df = pd.read_pickle(\n",
    "        DATA_DIR / \"spikesorting\" / \"recording_df\" / f\"{bird}.pickle.zip\"\n",
    "    )\n",
    "\n",
    "    # get units that are to be merged\n",
    "    merged_units = pd.read_pickle(\n",
    "            DATA_DIR / \"spikesorting\" / \"unit_clusters\" / f\"{bird}.pickle.zip\"\n",
    "        )\n",
    "    merged_units = merged_units.sort_values(by='n_playbacks', ascending=False)\n",
    "    merged_units = merged_units[merged_units.n_playbacks.astype('float')  > min_playbacks]\n",
    "    \n",
    "    # subset only units that are categorical\n",
    "    categorical_merged_units_mask = np.isin(merged_units.cluster_id.values, categorical_units_df[categorical_units_df['bird'] == bird].unit.astype(int).values)\n",
    "    merged_units = merged_units[categorical_merged_units_mask]\n",
    "    \n",
    "    plot = True\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(ncols=2, figsize=(10,2))\n",
    "        axs[0].hist(np.log10(merged_units.n_playbacks.values.astype('float')+1), bins = 100);\n",
    "        axs[0].set_title('log # playbacks')\n",
    "        axs[1].hist(np.log10(merged_units.n_spikes.values.astype('float')+1), bins = 100);\n",
    "        axs[1].set_title('log # spikes')\n",
    "        plt.show()\n",
    "        print('{} total # merged units: {}'.format(bird, len(merged_units)))\n",
    "\n",
    "    ensure_dir(spikesorting_folder / 'unit_spikerate' / statistics_timestamp)\n",
    "    \n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "        all_unit_spikerates = Parallel(n_jobs = 10, verbose=10)(\n",
    "            delayed(get_unit_cued_spike_rate_and_variance)(\n",
    "                unit_to_analyze,\n",
    "                spikesorting_folder,\n",
    "                statistics_timestamp,\n",
    "                bird = bird,\n",
    "                recompute=True,\n",
    "                parallel=True,\n",
    "\n",
    "                        )\n",
    "            for uniti, unit_to_analyze in tqdm(\n",
    "                merged_units.iterrows(), desc=\"unit\", total=len(merged_units)\n",
    "            )\n",
    "        );\n",
    "        all_unit_spikerates = pd.concat(all_unit_spikerates)\n",
    "        ensure_dir(DATA_DIR / 'spikerate_variance')\n",
    "        all_unit_spikerates.to_csv(DATA_DIR / 'spikerate_variance' / f\"{bird}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2984f9",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe0f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab7823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want a dataframe of [unit, stimulus, cue, mean_spikerate, variance_spikerate, n_trials]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5330a958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "    all_unit_spikerates = Parallel(n_jobs = 10, verbose=10)(\n",
    "        delayed(get_unit_cued_spike_rate_and_variance)(\n",
    "            unit_to_analyze,\n",
    "            spikesorting_folder,\n",
    "            statistics_timestamp,\n",
    "            bird = bird,\n",
    "            recompute=True,\n",
    "            parallel=True,\n",
    "\n",
    "                    )\n",
    "        for uniti, unit_to_analyze in tqdm(\n",
    "            merged_units.iterrows(), desc=\"unit\", total=len(merged_units)\n",
    "        )\n",
    "    );\n",
    "    all_unit_spikerates = pd.concat(all_unit_spikerates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62acb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_unit_spikerates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2bfb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unit_spikerates[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f1eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unit_spikerates_pivot = all_unit_spikerates.pivot_table(index=['stim_id', 'unit'], columns='cue_valid', values=['mean', 'var', 'count'], aggfunc='first')\n",
    "all_unit_spikerates_pivot.columns = ['_'.join(col).strip() for col in all_unit_spikerates_pivot.columns.values]\n",
    "df_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff52034",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_unit_spikerates_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "cues = [i.split('_')[0] for i in all_unit_spikerates.stim_id.values]\n",
    "stims = [i.split('_')[1] for i in all_unit_spikerates.stim_id.values]\n",
    "all_unit_spikerates['cues'] = cues\n",
    "all_unit_spikerates['stims'] = stims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e41041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unit_spikerates['cues'] = cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unit_spikerates[(all_unit_spikerates.unit == \"B1597_100\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a0e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unit_spikerates_pivot[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b32c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20655a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unit_spikerates_pivot = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6614940",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unit_spikerates[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21bab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unit_spikerates.cue_valid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unit_spikerates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdcp_paper",
   "language": "python",
   "name": "cdcp_paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
